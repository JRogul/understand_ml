{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from utils import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_y_true = np.array([0,0,0,0,1,0,0,1,1,1])\n",
    "cat_y_pred= np.array([0,0,0,0,0,1,1,1,1,1])\n",
    "lin_y_true = np.array([100, -100, 0, 200])\n",
    "lin_y_pred = np.array([-100, -100, -100, -100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Function\n",
    "\n",
    "The provided function `precision(y_true, y_pred)` calculates the precision for each class in a multiclass classification problem. \n",
    "\n",
    "## Definition of Precision\n",
    "\n",
    "Precision, also known as the positive predictive value, is a metric that measures the proportion of predicted positives that are actually positive. In binary classification, precision is defined as the number of true positives (TP) over the number of true positives plus the number of false positives (FP). This can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "True positives are instances of the positive class that the model correctly predicted as positive. False positives are instances of the negative class that the model incorrectly predicted as positive.\n",
    "\n",
    "## Precision in Multiclass Classification\n",
    "\n",
    "In the context of multiclass classification, we define precision for each class separately. For a given class, true positives are instances of that class that the model correctly predicted as that class, while false positives are instances of other classes that the model incorrectly predicted as the given class.\n",
    "\n",
    "This is done by iterating over each class and calculating the precision for that class. The function returns a list of precision values, one for each class.\n",
    "\n",
    "In the case where there are no predicted instances of a particular class in the test set (`len(total_preds) = 0`), the precision is defined to be 0 for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 1 1 1]\n",
      "[0 0 0 0 0 1 1 1 1 1]\n",
      "[0.8 0.6]\n"
     ]
    }
   ],
   "source": [
    "print(cat_y_true)\n",
    "print(cat_y_pred)\n",
    "print(metrics.precision(cat_y_true , cat_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall Function\n",
    "\n",
    "The provided function `recall(y_true, y_pred)` calculates the recall for each class in a multiclass classification problem. \n",
    "\n",
    "## Definition of Recall\n",
    "\n",
    "Recall, also known as sensitivity, hit rate, or true positive rate (TPR), is a metric that measures the proportion of actual positives that are correctly identified as such. In binary classification, recall is defined as the number of true positives (TP) over the number of true positives plus the number of false negatives (FN). This can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "True positives are instances of the positive class that the model correctly predicted as positive. False negatives are instances of the positive class that the model incorrectly predicted as negative.\n",
    "\n",
    "## Recall in Multiclass Classification\n",
    "\n",
    "In the context of multiclass classification, we define recall for each class separately. For a given class, true positives are instances of that class that the model correctly predicted as that class, while false negatives are instances of that class that the model incorrectly predicted as some other class.\n",
    "\n",
    "This is done by iterating over each class and calculating the recall for that class. The function returns a list of recall values, one for each class.\n",
    "\n",
    "In the case where there are no true instances of a particular class in the test set (`total_true = 0`), the recall is defined to be 0 for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 1 1 1]\n",
      "[0 0 0 0 0 1 1 1 1 1]\n",
      "[0.66666667 0.75      ]\n"
     ]
    }
   ],
   "source": [
    "print(cat_y_true)\n",
    "print(cat_y_pred)\n",
    "print(metrics.recall(cat_y_true , cat_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score Function\n",
    "\n",
    "The provided function `f1_score(y_true, y_pred)` calculates the F1 score for each class in a multiclass classification problem.\n",
    "\n",
    "## Definition of F1 Score\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, and it serves as a balance between these two metrics. An F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. In binary classification, the F1 score is defined as:\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} =  2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "In the context of multiclass classification, we calculate the F1 score for each class separately. For a given class, the F1 score uses the precision and recall for that class.\n",
    "\n",
    "In the `f1_score` function, the precision and recall for each class are calculated using the previously defined `precision` and `recall` functions. The function then calculates the F1 score for each class, and it returns a list of F1 scores, one for each class.\n",
    "\n",
    "The `1e-8` in the denominator is a small number added to prevent division by zero when both precision and recall are zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 1 1 1]\n",
      "[0 0 0 0 0 1 1 1 1 1]\n",
      "[0.72727272 0.66666666]\n"
     ]
    }
   ],
   "source": [
    "print(cat_y_true)\n",
    "print(cat_y_pred)\n",
    "print(metrics.f1_score(cat_y_true , cat_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
