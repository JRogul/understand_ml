{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from utils import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_y_true = np.array([0,0,0,0,1,0,0,1,1,1])\n",
    "cat_y_pred= np.array([0,0,0,0,0,1,1,1,1,1])\n",
    "lin_y_true = np.array([100, -100, 0, 200])\n",
    "lin_y_pred = np.array([-100, -100, -100, -100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Measurements Function\n",
    "\n",
    "Function `compute_measurements(conf_matrix)` calculates the True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN) for each class in a multiclass classification problem. These measurements are fundamental in understanding the performance of a classification model.\n",
    "\n",
    "## Definition of TP, FP, FN, TN\n",
    "\n",
    "- True Positives (TP): These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n",
    "- True Negatives (TN): These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n",
    "- False Positives (FP): When actual class is no and predicted class is yes.\n",
    "- False Negatives (FN): When actual class is yes but predicted class in no.\n",
    "\n",
    "## Computation\n",
    "\n",
    "In the `compute_measurements` function, the measurements are calculated as follows:\n",
    "\n",
    "- TP: The diagonal elements of the confusion matrix.\n",
    "- FP: The sum of the elements in the column for the class, excluding the TP.\n",
    "- FN: The sum of the elements in the row for the class, excluding the TP.\n",
    "- TN: The sum of all elements in the confusion matrix, excluding the elements in the row and column for the class.\n",
    "\n",
    "The function returns these measurements as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4, 3], dtype=int64),\n",
       " array([1, 2], dtype=int64),\n",
       " array([2, 1], dtype=int64),\n",
       " array([3, 4], dtype=int64))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(cat_y_true, cat_y_pred)\n",
    "metrics.compute_measurements(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Function\n",
    "\n",
    "Function `precision(y_true, y_pred)` calculates the precision for each class in a multiclass classification problem. \n",
    "\n",
    "## Definition of Precision\n",
    "\n",
    "Precision, also known as the positive predictive value, is a metric that measures the proportion of predicted positives that are actually positive. In binary classification, precision is defined as the number of true positives (TP) over the number of true positives plus the number of false positives (FP). This can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "True positives are instances of the positive class that the model correctly predicted as positive. False positives are instances of the negative class that the model incorrectly predicted as positive.\n",
    "\n",
    "## Precision in Multiclass Classification\n",
    "\n",
    "In the context of multiclass classification, precision is defined for each class separately. For a given class, true positives are instances of that class that the model correctly predicted as that class, while false positives are instances of other classes that the model incorrectly predicted as the given class.\n",
    "\n",
    "This is done by iterating over each class and calculating the precision for that class. The function returns a list of precision values, one for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 1 1 1]\n",
      "[0 0 0 0 0 1 1 1 1 1]\n",
      "[0.8 0.6]\n"
     ]
    }
   ],
   "source": [
    "print(cat_y_true)\n",
    "print(cat_y_pred)\n",
    "print(metrics.precision(cat_y_true , cat_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall Function\n",
    "\n",
    "Function `recall(y_true, y_pred)` calculates the recall for each class in a multiclass classification problem. \n",
    "\n",
    "## Definition of Recall\n",
    "\n",
    "Recall, also known as sensitivity, hit rate, or true positive rate (TPR), is a metric that measures the proportion of actual positives that are correctly identified as such. In binary classification, recall is defined as the number of true positives (TP) over the number of true positives plus the number of false negatives (FN). This can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "True positives are instances of the positive class that the model correctly predicted as positive. False negatives are instances of the positive class that the model incorrectly predicted as negative.\n",
    "\n",
    "## Recall in Multiclass Classification\n",
    "\n",
    "In the context of multiclass classification,recall is definied for each class separately. For a given class, true positives are instances of that class that the model correctly predicted as that class, while false negatives are instances of that class that the model incorrectly predicted as some other class.\n",
    "\n",
    "This is done by iterating over each class and calculating the recall for that class. The function returns a list of recall values, one for each class.\n",
    "\n",
    "In the case where there are no true instances of a particular class in the test set (`total_true = 0`), the recall is defined to be 0 for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 1 1 1]\n",
      "[0 0 0 0 0 1 1 1 1 1]\n",
      "[0.66666667 0.75      ]\n"
     ]
    }
   ],
   "source": [
    "print(cat_y_true)\n",
    "print(cat_y_pred)\n",
    "print(metrics.recall(cat_y_true , cat_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score Function\n",
    "\n",
    "Function `f1_score(y_true, y_pred)` calculates the F1 score for each class in a multiclass classification problem.\n",
    "\n",
    "## Definition of F1 Score\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, and it serves as a balance between these two metrics. An F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. In binary classification, the F1 score is defined as:\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} =  2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "In the context of multiclass classification,F1 score is calculated for each class separately. For a given class, the F1 score uses the precision and recall for that class.\n",
    "\n",
    "In the `f1_score` function, the precision and recall for each class are calculated using the previously defined `precision` and `recall` functions. The function then calculates the F1 score for each class, and it returns a list of F1 scores, one for each class.\n",
    "\n",
    "The `1e-8` in the denominator is a small number added to prevent division by zero when both precision and recall are zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 1 1 1]\n",
      "[0 0 0 0 0 1 1 1 1 1]\n",
      "[0.72727272 0.66666666]\n"
     ]
    }
   ],
   "source": [
    "print(cat_y_true)\n",
    "print(cat_y_pred)\n",
    "print(metrics.f1_score(cat_y_true , cat_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix Function\n",
    "\n",
    "Function `confusion_matrix(y_true, y_pred, return_perf_measurement=False)` calculates the confusion matrix for a multiclass classification problem. The confusion matrix is a table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class (or vice versa). \n",
    "\n",
    "## Parameters\n",
    "\n",
    "- `y_true`: Actual class labels.\n",
    "- `y_pred`: Predicted class labels.\n",
    "- `return_perf_measurement`: If True, function also returns performance measurements (True Positives, False Positives, False Negatives, True Negatives). Default is False.\n",
    "\n",
    "## Returns\n",
    "\n",
    "- If `return_perf_measurement` is False, the function returns only the confusion matrix.\n",
    "- If `return_perf_measurement` is True, the function returns a tuple containing the confusion matrix and performance measurements for each class.\n",
    "\n",
    "In the confusion matrix, the element at the i-th row and j-th column of the matrix indicates the number of instances that belong to the i-th actual class and are predicted as the j-th class.\n",
    "\n",
    "The function iterates over each unique class and for each class, it calculates the number of instances that belong to that class in the actual labels (`y_true`) and are predicted as each class in the predicted labels (`y_pred`). This is done for every combination of actual and predicted classes to form the confusion matrix.\n",
    "\n",
    "The function optionally returns the performance measurements (True Positives, False Positives, False Negatives, True Negatives) for each class if `return_perf_measurement` is set to True. This is done by calling the function `compute_measurements(conf_matrix)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [1., 0., 1.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_y_true = [1, 2, 3, 1, 2, 3]\n",
    "cat_y_pred= [1, 2, 1, 1, 2, 3]\n",
    "metrics.confusion_matrix(cat_y_true, cat_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2., 0., 0.],\n",
       "        [0., 2., 0.],\n",
       "        [1., 0., 1.]]),\n",
       " (array([2., 2., 1.]),\n",
       "  array([1., 0., 0.]),\n",
       "  array([0., 0., 1.]),\n",
       "  array([3., 4., 4.])))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conf matrix, TP, FP, FN, TN\n",
    "metrics.confusion_matrix(cat_y_true, cat_y_pred, return_perf_measurement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error Function\n",
    "\n",
    "Function `mean_squared_error(y_true, y_pred, root=False)` calculates the Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) between the true and predicted values. \n",
    "\n",
    "## Definition of MSE and RMSE\n",
    "\n",
    "- Mean Squared Error (MSE): It is the average of the squared differences between the predicted and actual values. It is a popular metric for regression problems and it is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_{\\text{true},i} - y_{\\text{pred},i})^2\n",
    "$$\n",
    "\n",
    "- Root Mean Squared Error (RMSE): It is the square root of the MSE. It measures the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are, and RMSE is a measure of how spread out these residuals are.\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "## Computation\n",
    "\n",
    "In the `mean_squared_error` function, the MSE is calculated as the mean of the squared differences between `y_true` and `y_pred`. If `root` is set to True, the function returns the RMSE, which is the square root of the MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 100 -100    0  200]\n",
      "[-100 -100 -100 -100]\n",
      "Mean squared error: 35000.0\n",
      "Root mean squared error: 187.1\n"
     ]
    }
   ],
   "source": [
    "print(lin_y_true)\n",
    "print(lin_y_pred)\n",
    "mse = metrics.mean_squared_error(lin_y_true, lin_y_pred)\n",
    "rmse = metrics.mean_squared_error(lin_y_true, lin_y_pred, root=True)\n",
    "print(f'Mean squared error: {mse:.1f}')\n",
    "print(f'Root mean squared error: {rmse:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Error Function\n",
    "\n",
    "Function `mean_absolute_error(y_true, y_pred)` calculates the Mean Absolute Error (MAE) between the true and predicted values. \n",
    "\n",
    "## Definition of MAE\n",
    "\n",
    "Mean Absolute Error (MAE) is the average of the absolute differences between the predicted and actual values. It measures the average magnitude of errors in a set of predictions, without considering their direction. It's a popular metric for regression problems and it is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}\\left|y_{\\text{true},i} - y_{\\text{pred},i}\\right|\n",
    "$$\n",
    "\n",
    "## Computation\n",
    "\n",
    "In the `mean_absolute_error` function, the MAE is calculated as the mean of the absolute differences between `y_true` and `y_pred`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 100 -100    0  200]\n",
      "[-100 -100 -100 -100]\n",
      "Mean absolute error: 150.0\n"
     ]
    }
   ],
   "source": [
    "print(lin_y_true)\n",
    "print(lin_y_pred)\n",
    "mae = metrics.mean_absolute_error(lin_y_true, lin_y_pred)\n",
    "print(f'Mean absolute error: {mae:.1f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
